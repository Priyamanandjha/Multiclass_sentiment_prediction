{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset contains two columns first contains the tweets and the second contains four class (figurative, irony, sarcasm, regular ) we have to predict the class of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential #(used to initialise the nural network)\n",
    "from keras.layers import Dense #(used to create the layers in ANN)\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1  #sarcasm for #people who don't understand #diy...  figurative\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3  @wilw Why do I get the feeling you like games?...  figurative\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...  figurative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweets    0\n",
       "class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81408, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "figurative    21238\n",
       "irony         20894\n",
       "sarcasm       20681\n",
       "regular       18595\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "corpus = []\n",
    "for i in range(0, 81408):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', data['tweets'][i])\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the corpus\n",
    "np.save('tweet_cleaned_corpus.npy', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load corpus\n",
    "corpus = np.load('tweet_cleaned_corpus.npy', mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating TF/IDF matrix to calculate the term weight of the words.  \n",
    "max_df = 0.50 means \"It ignores terms that appear in more than 50% of the documents\".min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "min_df = 5 means \"ignore terms that appear in less than 5 documents\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, stop_words = {'english'},ngram_range=(1, 2),min_df=5, max_df = 0.3)\n",
    " \n",
    "X_tf = tfidf_vectorizer.fit_transform(corpus)\n",
    "y = data.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have 4 class categories so we are going to encode the output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_encoded = []\n",
    "for data_point in y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tf.toarray(), Y_encoded, test_size = 0.25, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=512, init = 'uniform', activation='sigmoid',input_dim=len(X_train[0])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(units=409, activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the model with input nodes equal to len(X_train[0])\n",
    "init = uniform will assign random weights to the layers initially.\n",
    "we have used activation function sigmoid in hidden layers as it will give continous output to next layer\n",
    "in output layer we have used softmax activation function as the output is multiclass classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,optimizer=SGD(lr=0.01,momentum=0.9, nesterov=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used SGD optimiser as the chances of getting local minima is very high, so taking the step by step approcah to get the minima with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### while compiling we are using \"categorical_crossentropy\" as the loss function as the output is multiclass(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 54950 samples, validate on 6106 samples\n",
      "Epoch 1/75\n",
      "54950/54950 [==============================] - 112s 2ms/step - loss: 1.3985 - accuracy: 0.2562 - val_loss: 1.3908 - val_accuracy: 0.2563\n",
      "Epoch 2/75\n",
      "54950/54950 [==============================] - 112s 2ms/step - loss: 1.3845 - accuracy: 0.2768 - val_loss: 1.3896 - val_accuracy: 0.4309\n",
      "Epoch 3/75\n",
      "54950/54950 [==============================] - 111s 2ms/step - loss: 1.3599 - accuracy: 0.3163 - val_loss: 1.3425 - val_accuracy: 0.2622\n",
      "Epoch 4/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 1.3064 - accuracy: 0.4016 - val_loss: 1.2627 - val_accuracy: 0.7270\n",
      "Epoch 5/75\n",
      "54950/54950 [==============================] - 113s 2ms/step - loss: 1.2157 - accuracy: 0.5224 - val_loss: 1.1611 - val_accuracy: 0.4720\n",
      "Epoch 6/75\n",
      "54950/54950 [==============================] - 111s 2ms/step - loss: 1.0903 - accuracy: 0.6118 - val_loss: 1.0359 - val_accuracy: 0.6105\n",
      "Epoch 7/75\n",
      "54950/54950 [==============================] - 113s 2ms/step - loss: 0.9652 - accuracy: 0.6689 - val_loss: 0.9100 - val_accuracy: 0.7330\n",
      "Epoch 8/75\n",
      "54950/54950 [==============================] - 113s 2ms/step - loss: 0.8649 - accuracy: 0.6884 - val_loss: 0.8253 - val_accuracy: 0.7216\n",
      "Epoch 9/75\n",
      "54950/54950 [==============================] - 112s 2ms/step - loss: 0.7915 - accuracy: 0.6953 - val_loss: 0.8437 - val_accuracy: 0.5064\n",
      "Epoch 10/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.7402 - accuracy: 0.6967 - val_loss: 0.7564 - val_accuracy: 0.7268\n",
      "Epoch 11/75\n",
      "54950/54950 [==============================] - 112s 2ms/step - loss: 0.7052 - accuracy: 0.7019 - val_loss: 0.6922 - val_accuracy: 0.7098\n",
      "Epoch 12/75\n",
      "54950/54950 [==============================] - 113s 2ms/step - loss: 0.6797 - accuracy: 0.7019 - val_loss: 0.6867 - val_accuracy: 0.6186\n",
      "Epoch 13/75\n",
      "54950/54950 [==============================] - 116s 2ms/step - loss: 0.6588 - accuracy: 0.7052 - val_loss: 0.6744 - val_accuracy: 0.7324\n",
      "Epoch 14/75\n",
      "54950/54950 [==============================] - 115s 2ms/step - loss: 0.6441 - accuracy: 0.7060 - val_loss: 0.6849 - val_accuracy: 0.5668\n",
      "Epoch 15/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.6315 - accuracy: 0.7078 - val_loss: 0.6544 - val_accuracy: 0.6238\n",
      "Epoch 16/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.6216 - accuracy: 0.7087 - val_loss: 0.6311 - val_accuracy: 0.7326\n",
      "Epoch 17/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.6131 - accuracy: 0.7091 - val_loss: 0.6286 - val_accuracy: 0.7286\n",
      "Epoch 18/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.6064 - accuracy: 0.7098 - val_loss: 0.6201 - val_accuracy: 0.6915\n",
      "Epoch 19/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.6005 - accuracy: 0.7102 - val_loss: 0.6108 - val_accuracy: 0.7064\n",
      "Epoch 20/75\n",
      "54950/54950 [==============================] - 115s 2ms/step - loss: 0.5939 - accuracy: 0.7138 - val_loss: 0.6082 - val_accuracy: 0.7054\n",
      "Epoch 21/75\n",
      "54950/54950 [==============================] - 115s 2ms/step - loss: 0.5896 - accuracy: 0.7128 - val_loss: 0.6294 - val_accuracy: 0.7358\n",
      "Epoch 22/75\n",
      "54950/54950 [==============================] - 113s 2ms/step - loss: 0.5851 - accuracy: 0.7147 - val_loss: 0.6203 - val_accuracy: 0.7018\n",
      "Epoch 23/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.5817 - accuracy: 0.7151 - val_loss: 0.6227 - val_accuracy: 0.6497\n",
      "Epoch 24/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.5774 - accuracy: 0.7143 - val_loss: 0.6060 - val_accuracy: 0.6738\n",
      "Epoch 25/75\n",
      "54950/54950 [==============================] - 114s 2ms/step - loss: 0.5740 - accuracy: 0.7164 - val_loss: 0.6054 - val_accuracy: 0.7239\n",
      "Epoch 26/75\n",
      "54950/54950 [==============================] - 117s 2ms/step - loss: 0.5701 - accuracy: 0.7197 - val_loss: 0.5994 - val_accuracy: 0.6978\n",
      "Epoch 27/75\n",
      "54950/54950 [==============================] - 116s 2ms/step - loss: 0.5668 - accuracy: 0.7213 - val_loss: 0.5952 - val_accuracy: 0.7129\n",
      "Epoch 28/75\n",
      "54950/54950 [==============================] - 106s 2ms/step - loss: 0.5635 - accuracy: 0.7226 - val_loss: 0.5998 - val_accuracy: 0.6806\n",
      "Epoch 29/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5610 - accuracy: 0.7200 - val_loss: 0.5981 - val_accuracy: 0.7180\n",
      "Epoch 30/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5571 - accuracy: 0.7218 - val_loss: 0.6040 - val_accuracy: 0.7312\n",
      "Epoch 31/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5561 - accuracy: 0.7234 - val_loss: 0.6088 - val_accuracy: 0.6539\n",
      "Epoch 32/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5528 - accuracy: 0.7253 - val_loss: 0.5943 - val_accuracy: 0.6959\n",
      "Epoch 33/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5503 - accuracy: 0.7263 - val_loss: 0.5954 - val_accuracy: 0.6841\n",
      "Epoch 34/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5467 - accuracy: 0.7262 - val_loss: 0.5982 - val_accuracy: 0.6969\n",
      "Epoch 35/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5441 - accuracy: 0.7300 - val_loss: 0.6203 - val_accuracy: 0.7365\n",
      "Epoch 36/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5427 - accuracy: 0.7296 - val_loss: 0.6268 - val_accuracy: 0.7352\n",
      "Epoch 37/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5406 - accuracy: 0.7297 - val_loss: 0.6177 - val_accuracy: 0.6194\n",
      "Epoch 38/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5374 - accuracy: 0.7327 - val_loss: 0.6054 - val_accuracy: 0.6795\n",
      "Epoch 39/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5348 - accuracy: 0.7333 - val_loss: 0.6648 - val_accuracy: 0.5699\n",
      "Epoch 40/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5338 - accuracy: 0.7335 - val_loss: 0.5986 - val_accuracy: 0.7050\n",
      "Epoch 41/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5303 - accuracy: 0.7354 - val_loss: 0.5960 - val_accuracy: 0.6937\n",
      "Epoch 42/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5286 - accuracy: 0.7372 - val_loss: 0.5972 - val_accuracy: 0.6847\n",
      "Epoch 43/75\n",
      "54950/54950 [==============================] - 105s 2ms/step - loss: 0.5261 - accuracy: 0.7365 - val_loss: 0.6097 - val_accuracy: 0.6354\n",
      "Epoch 44/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5246 - accuracy: 0.7378 - val_loss: 0.6109 - val_accuracy: 0.6415\n",
      "Epoch 45/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5211 - accuracy: 0.7401 - val_loss: 0.6072 - val_accuracy: 0.6428\n",
      "Epoch 46/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5192 - accuracy: 0.7410 - val_loss: 0.6001 - val_accuracy: 0.7090\n",
      "Epoch 47/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5175 - accuracy: 0.7431 - val_loss: 0.6050 - val_accuracy: 0.7221\n",
      "Epoch 48/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5155 - accuracy: 0.7426 - val_loss: 0.6083 - val_accuracy: 0.6561\n",
      "Epoch 49/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5123 - accuracy: 0.7462 - val_loss: 0.6064 - val_accuracy: 0.6972\n",
      "Epoch 50/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5107 - accuracy: 0.7464 - val_loss: 0.6023 - val_accuracy: 0.6656\n",
      "Epoch 51/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5080 - accuracy: 0.7488 - val_loss: 0.6205 - val_accuracy: 0.6633\n",
      "Epoch 52/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5058 - accuracy: 0.7494 - val_loss: 0.6099 - val_accuracy: 0.7181\n",
      "Epoch 53/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.5044 - accuracy: 0.7503 - val_loss: 0.6448 - val_accuracy: 0.5745\n",
      "Epoch 54/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.5019 - accuracy: 0.7548 - val_loss: 0.6057 - val_accuracy: 0.6885\n",
      "Epoch 55/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4996 - accuracy: 0.7551 - val_loss: 0.6242 - val_accuracy: 0.6525\n",
      "Epoch 56/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4973 - accuracy: 0.7545 - val_loss: 0.6120 - val_accuracy: 0.7114\n",
      "Epoch 57/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4954 - accuracy: 0.7551 - val_loss: 0.6376 - val_accuracy: 0.7321\n",
      "Epoch 58/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4929 - accuracy: 0.7597 - val_loss: 0.6267 - val_accuracy: 0.6151\n",
      "Epoch 59/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4912 - accuracy: 0.7603 - val_loss: 0.6199 - val_accuracy: 0.7057\n",
      "Epoch 60/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4891 - accuracy: 0.7604 - val_loss: 0.6222 - val_accuracy: 0.7082\n",
      "Epoch 61/75\n",
      "54950/54950 [==============================] - 105s 2ms/step - loss: 0.4866 - accuracy: 0.7624 - val_loss: 0.6428 - val_accuracy: 0.7203\n",
      "Epoch 62/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4846 - accuracy: 0.7647 - val_loss: 0.6179 - val_accuracy: 0.6782\n",
      "Epoch 63/75\n",
      "54950/54950 [==============================] - 106s 2ms/step - loss: 0.4833 - accuracy: 0.7647 - val_loss: 0.6457 - val_accuracy: 0.6492\n",
      "Epoch 64/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4808 - accuracy: 0.7663 - val_loss: 0.6240 - val_accuracy: 0.6282\n",
      "Epoch 65/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4781 - accuracy: 0.7680 - val_loss: 0.6316 - val_accuracy: 0.6702\n",
      "Epoch 66/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4748 - accuracy: 0.7703 - val_loss: 0.6217 - val_accuracy: 0.6608\n",
      "Epoch 67/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4744 - accuracy: 0.7694 - val_loss: 0.6287 - val_accuracy: 0.6323\n",
      "Epoch 68/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4725 - accuracy: 0.7714 - val_loss: 0.6271 - val_accuracy: 0.6379\n",
      "Epoch 69/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4695 - accuracy: 0.7736 - val_loss: 0.6385 - val_accuracy: 0.6228\n",
      "Epoch 70/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4671 - accuracy: 0.7751 - val_loss: 0.6326 - val_accuracy: 0.6915\n",
      "Epoch 71/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4658 - accuracy: 0.7755 - val_loss: 0.6390 - val_accuracy: 0.6394\n",
      "Epoch 72/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4637 - accuracy: 0.7762 - val_loss: 0.7151 - val_accuracy: 0.5423\n",
      "Epoch 73/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4628 - accuracy: 0.7773 - val_loss: 0.6432 - val_accuracy: 0.6101\n",
      "Epoch 74/75\n",
      "54950/54950 [==============================] - 103s 2ms/step - loss: 0.4598 - accuracy: 0.7789 - val_loss: 0.6367 - val_accuracy: 0.6626\n",
      "Epoch 75/75\n",
      "54950/54950 [==============================] - 104s 2ms/step - loss: 0.4569 - accuracy: 0.7812 - val_loss: 0.6510 - val_accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "#Fitting the ANN to the training set \n",
    "history = model.fit(np.array(X_train), np.array(y_train), epochs=75, batch_size=64, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when we fit the model with the training set we are giving validation split =0.1 i.e. the model will get validated in 10% of the data, the batch size is 64 so after every 64 rows the weight of the neural network is getting updated, the epochs can be changed to run the whole dataset again so that it can decrease the loss and increase the accuracy.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save(\"tweet_predictor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [np.argmax(pred) for pred in model.predict(np.array(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               12939264  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 12,941,316\n",
      "Trainable params: 12,941,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 14049\n",
      "Total: 20352\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == np.argmax(y_test[i]):\n",
    "        correct += 1\n",
    "print (\"Correct:\", correct)\n",
    "print (\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
